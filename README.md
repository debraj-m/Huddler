# Enhanced Huddler Pipeline

A sophisticated natural language processing pipeline for detecting questions and determining whether they're directed at a specific user in conversational contexts like meetings, interviews, and group discussions.

## ğŸš€ Features

### Core Components

- **Question Detection**: Identifies questions using multiple pattern-matching techniques and spaCy NLP
- **Answerability Classification**: Categorizes questions by type (factual, opinion, rhetorical, speculative, clarification)
- **Direction Detection**: Determines if questions are directed at a specific user with high accuracy
- **BERT Integration**: Optional BERT-based enhancement for ambiguous contextual cases
- **Comprehensive Testing**: Built-in test framework with real-world scenarios

### Key Capabilities

- âœ… Direct name addressing detection ("Sarah, can you explain...")
- âœ… Role-based addressing ("As a software engineer, what's your approach...")
- âœ… Contextual follow-up detection (questions after user input)
- âœ… Multi-question handling in single messages
- âœ… Rhetorical question filtering
- âœ… Expertise-based relevance matching
- âœ… Conversation history analysis
- âœ… Speaker pattern learning

## ğŸ“‹ Requirements

### Core Dependencies
```
spacy>=3.4.0
numpy>=1.21.0
```

### Optional BERT Dependencies
```
transformers>=4.20.0
torch>=1.12.0
```

### Installation

```bash
# Core installation
pip install spacy numpy

# Download spaCy English model
python -m spacy download en_core_web_sm

# Optional: BERT support
pip install transformers torch
```

## ğŸ› ï¸ Usage

### Basic Setup

```python
from enhanced_huddler_pipeline import EnhancedHuddlerPipeline

# Initialize the pipeline
pipeline = EnhancedHuddlerPipeline(
    user_name="Sarah",
    user_role="software engineer",
    user_context="Sarah is a software engineer with 5 years of experience...",
    meeting_context="Technical job interview...",
    use_bert=False,
    use_bert_direction=True  # Enable BERT for direction detection
)

# Process a message
text = "Sarah, can you tell us about your experience with distributed systems?"
speaker_id = 1
questions, answerability, direction = pipeline.process(text, speaker_id, pipeline.user_context)

print(f"Detected questions: {questions}")
print(f"Direction result: {direction}")
print(f"Should respond: {direction.directed_at_user}")
print(f"Confidence: {direction.confidence:.2f}")
```

### Advanced Usage with Conversation History

```python
# Build conversation history
pipeline.conversation_history = [
    {'role': 'third_party', 'speaker': 1, 'text': "Let's discuss system architecture."},
    {'role': 'user', 'text': "I prefer microservices architecture."},
]

# Process follow-up question
follow_up = "How would you handle data consistency in that approach?"
questions, answerability, direction = pipeline.process(follow_up, 1, pipeline.user_context)
```

## ğŸ§ª Testing

The system includes a comprehensive test framework covering various real-world scenarios:

```python
from enhanced_huddler_pipeline import create_comprehensive_test_suite

# Run all tests
summary = create_comprehensive_test_suite()
print(f"Tests passed: {summary['passed']}/{summary['total_tests']}")
```

### Test Scenarios Covered

- **Interview Questions**: Direct technical questions with name/role addressing
- **Status Meetings**: Progress updates and blocker discussions
- **Academic Presentations**: Technical follow-ups and clarifications
- **Multi-party Conversations**: Contextual addressing in group settings
- **Edge Cases**: Rhetorical questions, ambiguous "you" references

## ğŸ“Š Configuration Options

### Pipeline Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `user_name` | str | Required | User's name for direct addressing detection |
| `user_role` | str | Required | User's role for role-based addressing |
| `user_context` | str | Required | Detailed user background for expertise matching |
| `meeting_context` | str | Required | Context about the meeting/conversation |
| `use_bert` | bool | False | Enable BERT for answerability classification |
| `use_bert_direction` | bool | False | Enable BERT for direction detection |

### Confidence Thresholds

The system uses different confidence thresholds for various addressing types:

- **Direct Name**: 0.97 (very high confidence)
- **Role-based**: 0.92 (high confidence)
- **Contextual**: 0.85-0.95 (varies based on context strength)
- **General**: 0.9 (high confidence for exclusion)

## ğŸ—ï¸ Architecture

```
EnhancedHuddlerPipeline
â”œâ”€â”€ QuestionDetector
â”‚   â”œâ”€â”€ Pattern-based detection
â”‚   â””â”€â”€ spaCy sentence segmentation
â”œâ”€â”€ AnswerabilityClassifier
â”‚   â”œâ”€â”€ Rule-based classification
â”‚   â””â”€â”€ Optional BERT enhancement
â””â”€â”€ EnhancedDirectionDetector
    â”œâ”€â”€ Direct addressing (name/role)
    â”œâ”€â”€ Contextual analysis
    â”œâ”€â”€ Speaker pattern learning
    â””â”€â”€ Optional BERT validation
```

### Question Types

- **Factual**: Questions seeking objective information
- **Opinion**: Questions asking for subjective views
- **Rhetorical**: Questions not expecting answers
- **Speculative**: Hypothetical or theoretical questions
- **Clarification**: Questions seeking explanation or elaboration

### Addressing Types

- **Direct Name**: Explicit name mention ("Sarah, can you...")
- **Role-based**: Role-specific addressing ("As a developer...")
- **Contextual**: Implicit addressing based on conversation flow
- **General**: Questions for anyone in the group
- **None**: No clear addressing pattern

## ğŸ“ˆ Performance

### Accuracy Metrics (Test Suite Results)

- **Direct Addressing**: >95% accuracy
- **Contextual Detection**: ~85-90% accuracy
- **Rhetorical Filtering**: >90% accuracy
- **Overall Pipeline**: ~88% accuracy across all scenarios

### BERT Enhancement

When enabled, BERT provides:
- Improved contextual understanding for ambiguous cases
- Better handling of complex conversation flows
- Enhanced confidence scoring for edge cases

## ğŸ”§ Customization

### Adding Custom Patterns

```python
# Extend question patterns
detector = QuestionDetector()
detector.question_patterns.append(r'your_custom_pattern')

# Add custom addressing patterns
direction_detector = EnhancedDirectionDetector("user", "role")
# Modify patterns in _enhanced_contextual_analysis method
```

### Expertise Matching

Customize the expertise matching in `_check_expertise_match`:

```python
def _check_expertise_match(self, question: str) -> float:
    # Add your domain-specific keywords
    custom_keywords = ['your', 'domain', 'keywords']
    # Implement custom matching logic
```

## ğŸš¨ Error Handling

The system gracefully handles:
- Missing BERT dependencies (falls back to rule-based approach)
- Empty conversation history
- Malformed input messages
- Unknown speaker IDs

## ğŸ“ Logging and Debugging

Enable debug output for BERT direction detection:

```python
# BERT scores are logged to console when use_bert_direction=True
# Look for: [DEBUG] BERT entailment score for contextual follow-up: X.XXX
```

### Adding Test Cases

```python
test_cases.append(test_framework.create_test_case(
    description="Your test description",
    conversation_history=[...],
    test_message={'role': 'third_party', 'speaker': 1, 'text': "Your test message"},
    expected_outcome={'should_respond': True, 'min_confidence': 0.8}
))
```

## ğŸ™ Acknowledgments

- spaCy for natural language processing
- Hugging Face Transformers for BERT integration
- The open-source NLP community

## ğŸ“ Support

For questions, issues, or feature requests, please open an issue on the project repository.

---

**Note**: This system is designed for conversational AI applications where determining question direction is critical for appropriate response generation. It's particularly useful in meeting assistants, interview bots, and group discussion facilitators.